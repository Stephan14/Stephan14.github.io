---
layout:     post
title:      "GFS论文总结"
subtitle:   "读书笔记"
date:       2022-03-26 18:18:30
author:     "zoushengfu"
header-img: "img/bridge-g3d5aea30b_1920.jpeg"
tags:
    - 分布式系统
---

##  设计概述

### 设计预期
1. 组件故障处于常态，系统需要能迅速侦测、冗余并灰度失效组件
2. 系统能存储一定数量的大文件(通常是在100MB以上)，同时也支持存储小文件，但是不需要对小文件进行优化
3. 主要是2种读操作，分别是大规模的流式读取和小规模的随机读取(**如果对性能有要求，可以将小规模的随机读取合并并排序，之后按照顺序批量读取**)
4. 主要是2种写操作，分别是大规模的顺序追加写和小规模的随机位置写入
5. 系统必须高效并且行为是明确定义的
6. 高性能的稳定网络带宽比低延迟重要

### 接口
提供一套类似传统文件系统的API接口函数，虽然并不是严格按照POSIX等标准API的形式定义。另外还提供了快照和append功能，快照可以以很低的成本创建一个文件或者目录树的拷贝；append功能允许多个客户端同时对一个文件进行数据追加操作，同时保证每个客户端的追加操作都是原子的

### 架构
![](https://res.cloudinary.com/bytedance14/image/upload/v1648291657/blog/v2-4c657836ce1a5110d50727a0e27dc9c2_720w.jpg)
- Master服务器会给每个Chunk分配一个全局不变的全球唯一的64位Chunk标识，同时会管理所有的元数据，包括名字空间、访问控制信息、文件和Chunk的映射关系以及当前的Chunk的位置信息，还管理着Chunk租用关系、孤儿Chunk的回收，以及Chunk的数据迁移
- Chunk服务器以linux文件的形式保存在本地硬盘上，并根据制定的chunk标识和字节范围来读写数据块，文件系统会缓存文件到内存中
- 客户端实现GFS文件系统的API接口函数、应用程序与Master节点和Chunk服务器的通讯、以及对数据进行读写操作。客户端只和Master节点获取元数据（会缓存），所有的数据操作都和Chunk服务进行交互。并且不缓存任何数据（Chunk服务器一样），因为大部分的程序要么以流的方式读取，要么数据太大无法被缓存

### 单一Master节点
优点:单一的Master节点可以通过全局的信息精确定位Chunk的位置以及进行复制决策
缺点:需要降低Master的读写操作避免Master节点成为系统的瓶颈
Master 节点管理着整个文件系统，主要涉及以下几个方面：

- 整个文件系统的元数据：包括命名空间（namespace）、访问控制信息（access control information）、文件与Chunk Server的映射关系以及每个 Chunk 的当前位置（the current locations of chunks）

- 文件系统的动态信息：Chunk 租用管理（chunk leases management）、孤儿 Chunk 的回收（garbage collection of orphaned chunks）以及Chunk 在 Chunk Server 之间的迁移

- 使用心跳周期性与每个 Chunk Server通信，发送至指令到各个 Chunk Server并接受 Chunk Server的状态信息

- 接受并回应客户端的操作请求

#### 简单读取的流程
1. 客户端把文件名和程序指定的字节偏移，根据固定的 Chunk大小，转换成文件的Chunk索引
2. 它把文件名和Chunk索引发送给Master节点
3. Master节点将相应的Chunk标识和副本的位置信息发还给客户端
4. 客户端用文件名和Chunk索引作为key缓存这些信息
5. 客户端会发送请求（包含Chunk的标识和字节范围）到其中的一个副本处，一般会选择最近的
> 1. 在后续对这个Chunk的后续读取操作中，客户端不会跟Master再通讯，除非缓存的元信息过期或者文件被重新打开
> 2.客户端通常会在一次请求中查询多个Chunk信息，Master节点的回应也可能包含了紧跟着这些请求的Chunk的后面的Chunk信息

### Chunk尺寸
选择较大的Chunk尺寸的有几个优点
- 减少客户端和Master的节点通讯，客户端可以缓存更多的Chunk的数据
- 客户端可以对一个Chunk进行多次操作，通过与Chunk服务器保存较长时间的TCP连接来减少网络负载
- 减少Master节点需要保存的元数据的数量

但大的Chunk尺寸也是有其缺陷的，即小文件包含较少的Chunk甚至只有一个Chunk，当有许多的客户端对同一个小文件进行多次访问时，存储这些Chunk的Chunk服务器就会变成热点。这个问题可以通过增大复制因子来解决。

这里还有一个值得思考的地方，即GFS对数据的冗余是以Chunk为基本单位而不是机器（或者说文件）。以机器为单位进行冗余的优点是简单方便，但是伸缩性不好，不能充分利用资源；而以Chunk为基本单位，虽然Master节点上需要存更多的元数据，但一个Chunk的信息也就64字节左右，而Chunk本身的粒度又有64M这么大，加之在Master中Chunk的位置信息是不持久化的，所以这并不会给Master带来太多的负担。

### 元数据

Master服务器存储3种主要类型的元数据：
- 文件和Chunk的命名空间
- 文件和Chunk的映射关系
- 每个Chunk的存放位置
> 前2种会以记录变更日志的方式记录到操作系统的系统日志中，日志文件存储在本地磁盘上，同时日志会被复制到其他的Master机器上；对于第3种不会持久化


#### 内存中的数据结构
Master会周期性扫描自己保存的全部信息，用以实现Chunk垃圾回收、Chunk服务器数据重新复制、Chunk迁移实现的负载均衡以及磁盘使用状况的统计功能

#### Chunk位置信息
只有Chunk服务器才能最终确定一个Chunk是否在它的硬盘 上。我们从没有考虑过在Master服务器上维护一个这些信息的全局视图，因为Chunk服务器的错误可能会导致Chunk自动消失(比如，硬盘损坏了或者无法访问了)，亦或者操作人员可能会重命名一个Chunk服务器。

#### 操作日志
只有在元数据的变化日志被持久化之后，才对客户端是可见。**为了缩短Master的启动时间，日志在增长到一定的数量的时候会对系统做一个Checkpoint（为了不阻塞正在进行的操作，会使用独立线程进行持久化），将所有的状态数据写入到一个Checkpoint文件，在恢复的时候Master服务器通过从磁盘上读取Checkpoint并回放Checkpoint之后的有限个日志文件就能恢复**

### 一致性模型
|  | 随机写 | 追加写 |
| :-----| ----: | :----: |
| 串行成功 | 已定义 | 已定义，但部分不一致 |
| 并行成功 | 一致但是未定义 | 已定义，但部分不一致 |
| 失败 | 不一致 | 不一致|

> 一致是指当客户端需要读取文件内某一个区域时，无论客户端是从哪个副本读的，看到的内容都是一样的
> 已定义的前提是一致的，同时需要满足当客户端写入了一块内容后再去读，可以读到之前写入的全部内容

下面通过几个例子详细解释一致和已定义的区别，以及随机写和追加写的一致性问题。这里有一个前提条件是客户端可能会有重试，即无论是顺序成功还是并行成功，都会有这种情况：即客户端某一次写入失败了，而后重试若干次最后写入成功。这种情况下也属于写入成功，而不属于写入失败，只有重试后依然失败的才算写入失败

#### 随机写
随机写由客户端指定写入的位置和内容
##### 并行成功
由于客户端知道写入的位置，所以无论是否存在重试写入位置和内容都是固定的，并且同一个chunk下所有副本写入的顺序又是一致的，所以只要最后全部写入都成功了那么文件最终一定是一致的。但并行成功是未定义的，因为客户端的写入范围可能会有交叉的情况，考虑下面这个例子：

初始状态下，文件内容为

| 位置 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :-----| ----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| 字节 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |

此时存在两个客户端C1和C2，其中C1写入的位置和内容是[4, "11111"]，C2写入的位置和内容是[7, "222"]，由于C1和C2的写入顺序不是固定的（同一个chunk由主chunk决定顺序）

Case1：假如C1先于C2执行，且写入同时完成，文件内容最终为

| 位置 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :-----| ----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| 字节 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 2 | 2 | 0 |

Case2：假如C2先于C1执行，且写入同时完成，文件内容最终为

| 位置 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :-----| ----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| 字节 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 2 | 0 |

甚至还有一种极端的情况（Case3）：假如chunk大小为8，那么C1和C2都需要写入chunk1和chunk2，而每个chunk又由自己的主chunk决定顺序，所以很可能发生chunk1中C1先于C2写入，而chunk2中C2先于C1写入，最后写入同时完成，那么最终文件内容为

| 位置 | 0(chunk1) | 1(chunk1) | 2(chunk1) | 3(chunk1) | 4(chunk1) | 5(chunk1) | 6(chunk1) | 7(chunk1) | 8(chunk2) | 9(chunk2) | 10(chunk2) |
| :-----| ----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| 字节 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 1 | 2 | 0 |

那么如果站在客户端的角度，实际上在写入完成之后并不一定可以看到自己写入的全部内容，例如Case1中只有C2可以看到自己写入的全部内容，Case2中只有C1能看到自己写入的全部内容，Case3中每个客户端都看不到自己写入的全部内容

##### 顺序成功
同并行成功，顺序成功无论是否有重试也不会影响最终的一致性。而顺序成功意味着只有一个写者在写入文件，显然写者在写入后可以看到自己写入的全部内容，所以是已定义的

##### 写入失败
由于失败可能发生在任何时候，假如写chunk一半时失败、前面的chunks写入成功后面某个chunk写入失败等等，所以一旦失败就会发生不一致

#### 追加写
在GFS中，追加写的写操作是原子的

##### 并行成功/顺序成功

由于追加写是原子的，所以每个追加写的范围是两两不重叠的，即每个写者写入成功后都可以看到自己写入的全部内容，所以追加写的写入是已定义的，与是否并行无关

又因为追加写的特性是每次在文件结尾写，即每次写的位置都是文件末尾，所以客户端一旦发生重试那么之前写入的内容就会不一致了，考虑下面这种情况

初始状态下，文件内容为"000"，并且存在3个副本，此时文件是一致的
- 副本1（主chunk）
| 位置 | 0 | 1 | 2 |
| :-----| ----: | :----: | :----: |
| 字节 | 0 | 0 | 0 |

- 副本2
| 位置 | 0 | 1 | 2 |
| :-----| ----: | :----: | :----: |
| 字节 | 0 | 0 | 0 |

- 副本3
| 位置 | 0 | 1 | 2 |
| :-----| ----: | :----: | :----: |
| 字节 | 0 | 0 | 0 |

此时发生追加写"aaa"，并且主chunk写入成功了，但在副本2写到一半时失败了，此时文件内容为

- 副本1（主chunk）
| 位置 | 0 | 1 | 2 | 3 | 4 | 5 |
| :-----| ----: | :----: | :----: | :-----| :-----| :-----|
| 字节 | 0 | 0 | 0 | a | a | a |

- 副本2
| 位置 | 0 | 1 | 2 | 3 | 4 |
| :-----| ----: | :----: | :----: | :----: | :----: |
| 字节 | 0 | 0 | 0 | a | a |

- 副本3
| 位置 | 0 | 1 | 2 |
| :-----| ----: | :----: | :----: |
| 字节 | 0 | 0 | 0 |

接着客户端感知到错误，并且发起重试并且重试成功。由于每个副本的写入位置需要一致，所以当客户端重试时，副本2和副本3需要先填充特殊字符让长度和主chunk一致，接着再写入最新的内容，此时文件内容为

- 副本1（主chunk）
| 位置 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
| :-----| ----: | :----: | :----: | :-----| :-----| :-----| :-----|:-----|:-----|
| 字节 | 0 | 0 | 0 | a | a | a | a | a | a |

- 副本2
| 位置 | 0 | 1 | 2 | 3 | 4 | 5 |6 | 7 | 8 |
| :-----| ----: | :----: | :----: | :----: | :----: | :----: |:----: |:----: |:----: |
| 字节 | 0 | 0 | 0 | a | a | x | a | a | a |

- 副本3
| 位置 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
| :-----| ----: | :----: | :----: | :---: | :---: | :---: | :---: | :---: | :----: |
| 字节 | 0 | 0 | 0 | x | x | x | a | a | a |

此时三个副本在[3, 5]范围内是不一致的。
所以GFS要求客户端在追加写时还需要额外做一些控制信息来保证读出的数据是正确的，详见论文的2.7.2 Implications for Applications小节

##### 写入失败
和随机写一样，只要失败就会发生不一致
