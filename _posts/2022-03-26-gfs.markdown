---
layout:     post
title:      "GFS论文总结"
subtitle:   "读书笔记"
date:       2022-03-26 18:18:30
author:     "zoushengfu"
header-img: "img/demo2.jpeg"
tags:
    - 分布式系统
---


##  设计概述
### 设计预期
1. 组件故障处于常态，系统需要能迅速侦测、冗余并灰度失效组件
2. 系统能存储一定数量的大文件(通常是在100MB以上)，同时也支持存储小文件，但是不需要对小文件进行优化
3. 主要是2种读操作，分别是大规模的流式读取和小规模的随机读取(**如果对性能有要求，可以将小规模的随机读取合并并排序，之后按照顺序批量读取**)
4. 主要是2种写操作，分别是大规模的顺序追加写和小规模的随机位置写入
5. 系统必须高效并且行为是明确定义的
6. 高性能的稳定网络带宽比低延迟重要

### 接口
提供一套类似传统文件系统的API接口函数，虽然并不是严格按照POSIX等标准API的形式定义。另外还提供了快照和append功能，快照可以以很低的成本创建一个文件或者目录树的拷贝；append功能允许多个客户端同时对一个文件进行数据追加操作，同时保证每个客户端的追加操作都是原子的

### 架构
![](https://res.cloudinary.com/bytedance14/image/upload/v1648291657/blog/v2-4c657836ce1a5110d50727a0e27dc9c2_720w.jpg)
- Master服务器会给每个Chunk分配一个全局不变的全球唯一的64位Chunk标识，同时会管理所有的元数据，包括名字空间、访问控制信息、文件和Chunk的映射关系以及当前的Chunk的位置信息，还管理着Chunk租用关系、孤儿Chunk的回收，以及Chunk的数据迁移
- Chunk服务器以linux文件的形式保存在本地硬盘上，并根据制定的chunk标识和字节范围来读写数据块，文件系统会缓存文件到内存中
- 客户端实现GFS文件系统的API接口函数、应用程序与Master节点和Chunk服务器的通讯、以及对数据进行读写操作。客户端只和Master节点获取元数据（会缓存），所有的数据操作都和Chunk服务进行交互。并且不缓存任何数据（Chunk服务器一样），因为大部分的程序要么以流的方式读取，要么数据太大无法被缓存

### 单一Master节点
优点:单一的Master节点可以通过全局的信息精确定位Chunk的位置以及进行复制决策
缺点:需要降低Master的读写操作避免Master节点成为系统的瓶颈
Master 节点管理着整个文件系统，主要涉及以下几个方面：

- 整个文件系统的元数据：包括命名空间（namespace）、访问控制信息（access control information）、文件与Chunk Server的映射关系以及每个 Chunk 的当前位置（the current locations of chunks）

- 文件系统的动态信息：Chunk 租用管理（chunk leases management）、孤儿 Chunk 的回收（garbage collection of orphaned chunks）以及Chunk 在 Chunk Server 之间的迁移

- 使用心跳周期性与每个 Chunk Server通信，发送至指令到各个 Chunk Server并接受 Chunk Server的状态信息

- 接受并回应客户端的操作请求

#### 简单读取的流程
1. 客户端把文件名和程序指定的字节偏移，根据固定的 Chunk大小，转换成文件的Chunk索引
2. 它把文件名和Chunk索引发送给Master节点
3. Master节点将相应的Chunk标识和副本的位置信息发还给客户端
4. 客户端用文件名和Chunk索引作为key缓存这些信息
5. 客户端会发送请求（包含Chunk的标识和字节范围）到其中的一个副本处，一般会选择最近的
> 1. 在后续对这个Chunk的后续读取操作中，客户端不会跟Master再通讯，除非缓存的元信息过期或者文件被重新打开
> 2.客户端通常会在一次请求中查询多个Chunk信息，Master节点的回应也可能包含了紧跟着这些请求的Chunk的后面的Chunk信息

### Chunk尺寸
选择较大的Chunk尺寸的有几个优点
- 减少客户端和Master的节点通讯，客户端可以缓存更多的Chunk的数据
- 客户端可以对一个Chunk进行多次操作，通过与Chunk服务器保存较长时间的TCP连接来减少网络负载
- 减少Master节点需要保存的元数据的数量

但大的Chunk尺寸也是有其缺陷的，即小文件包含较少的Chunk甚至只有一个Chunk，当有许多的客户端对同一个小文件进行多次访问时，存储这些Chunk的Chunk服务器就会变成热点。这个问题可以通过增大复制因子来解决。

这里还有一个值得思考的地方，即GFS对数据的冗余是以Chunk为基本单位而不是机器（或者说文件）。以机器为单位进行冗余的优点是简单方便，但是伸缩性不好，不能充分利用资源；而以Chunk为基本单位，虽然Master节点上需要存更多的元数据，但一个Chunk的信息也就64字节左右，而Chunk本身的粒度又有64M这么大，加之在Master中Chunk的位置信息是不持久化的，所以这并不会给Master带来太多的负担。

### 元数据

Master服务器存储3种主要类型的元数据：
- 文件和Chunk的命名空间
- 文件和Chunk的映射关系
- 每个Chunk的存放位置
> 前2种会以记录变更日志的方式记录到操作系统的系统日志中，日志文件存储在本地磁盘上，同时日志会被复制到其他的Master机器上；对于第3种不会持久化


#### 内存中的数据结构
Master会周期性扫描自己保存的全部信息，用以实现Chunk垃圾回收、Chunk服务器数据重新复制、Chunk迁移实现的负载均衡以及磁盘使用状况的统计功能

#### Chunk位置信息
只有Chunk服务器才能最终确定一个Chunk是否在它的硬盘 上。我们从没有考虑过在Master服务器上维护一个这些信息的全局视图，因为Chunk服务器的错误可能会导致Chunk自动消失(比如，硬盘损坏了或者无法访问了)，亦或者操作人员可能会重命名一个Chunk服务器。

#### 操作日志
只有在元数据的变化日志被持久化之后，才对客户端是可见。**为了缩短Master的启动时间，日志在增长到一定的数量的时候会对系统做一个Checkpoint（为了不阻塞正在进行的操作，会使用独立线程进行持久化），将所有的状态数据写入到一个Checkpoint文件，在恢复的时候Master服务器通过从磁盘上读取Checkpoint并回放Checkpoint之后的有限个日志文件就能恢复**
